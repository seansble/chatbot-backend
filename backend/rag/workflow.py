"""LangGraph RAG ì›Œí¬í”Œë¡œìš° - í‰ê°€ì ê¸°ë°˜ ì¡°ê±´ë¶€ ì²˜ë¦¬"""

from typing import Dict, List, TypedDict, Literal, Optional
from langgraph.graph import StateGraph, END
import logging
import re

logger = logging.getLogger(__name__)


class RAGState(TypedDict):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ - í‰ê°€ í•„ë“œ ì¶”ê°€"""

    # ì…ë ¥
    query: str  # ì›ë³¸ ì§ˆë¬¸

    # ì „ì²˜ë¦¬ ë‹¨ê³„
    processed_query: str  # ì •ì œëœ ì§ˆë¬¸
    query_type: str  # ì§ˆë¬¸ ìœ í˜•
    is_blocked: bool  # ì°¨ë‹¨ ì—¬ë¶€

    # ê²€ìƒ‰ ë‹¨ê³„
    search_strategy: str  # ê²€ìƒ‰ ì „ëµ
    documents: List[Dict]  # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
    relevance_score: float  # RAG ê´€ë ¨ë„ ì ìˆ˜

    # í‰ê°€ ë‹¨ê³„ (ìƒˆë¡œ ì¶”ê°€)
    requirements: List[str]  # ì§ˆë¬¸ì˜ ìš”êµ¬ì‚¬í•­ ë¶„í•´
    coverage: Dict[str, bool]  # ìš”êµ¬ì‚¬í•­ ì»¤ë²„ë¦¬ì§€
    coverage_score: float  # ì»¤ë²„ë¦¬ì§€ ì ìˆ˜ (0-1)
    needs_enhancement: bool  # LLM ë³´ê°• í•„ìš” ì—¬ë¶€

    # ë‹µë³€ ìƒì„± ë‹¨ê³„
    answer_method: str  # ë‹µë³€ ìƒì„± ë°©ë²•
    context: str  # êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸
    raw_answer: str  # 1ì°¨ ë‹µë³€
    final_answer: str  # ìµœì¢… ë‹µë³€

    # ë©”íƒ€ë°ì´í„°
    confidence: float  # ì „ì²´ ì‹ ë¢°ë„
    iteration: int  # ì¬ì‹œë„ íšŸìˆ˜
    debug_path: List[str]  # ë””ë²„ê¹… ê²½ë¡œ


class ImprovedRAGWorkflow:
    def __init__(self, retriever):
        self.retriever = retriever
        self.workflow = self._build_workflow()

        # ì§ˆë¬¸ ìœ í˜•ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.query_patterns = {
            "calculation": ["ì–¼ë§ˆ", "ê¸ˆì•¡", "ê³„ì‚°", "ì›”ê¸‰", "ì¼ë‹¹"],
            "qualification": ["ìê²©", "ì¡°ê±´", "ê°€ëŠ¥", "ë ê¹Œ", "ë˜ë‚˜ìš”"],
            "procedure": ["ì–´ë–»ê²Œ", "ë°©ë²•", "ì ˆì°¨", "ì‹ ì²­", "ì„œë¥˜"],
            "specific_case": ["ì„ê¸ˆì²´ë¶ˆ", "ê¶Œê³ ì‚¬ì§", "ê³„ì•½ë§Œë£Œ", "ìœ¡ì•„íœ´ì§"],
        }

        # ì°¨ë‹¨ íŒ¨í„´ (ë³µì¡í•œ ì§ˆë¬¸ì€ ì œì™¸)
        self.block_patterns = [
            (r"ì–¼ë§ˆ.*ë°›", "ê¸ˆì•¡_ê³„ì‚°_ê¸ˆì§€"),
            (r"ê³„ì‚°.*í•´", "ê¸ˆì•¡_ê³„ì‚°_ê¸ˆì§€"),
            (r"\d+ë§Œì›.*ë°›", "ê¸ˆì•¡_ê³„ì‚°_ê¸ˆì§€"),
        ]

        # ëª…ë°±í•œ ë¯¸ìŠ¤ë§¤ì¹˜ íŒ¨í„´
        self.mismatch_patterns = [
            ("ë°˜ë³µìˆ˜ê¸‰", ["ì¬ì·¨ì—…", "ì´‰ì§„"]),
            ("ì´ì§í™•ì¸ì„œ", ["ê¶Œê³ ì‚¬ì§", "ìì§„í‡´ì‚¬"]),
            ("65ì„¸", ["ì„ê¸ˆì²´ë¶ˆ", "í”„ë¦¬ëœì„œ"]),
            ("ëª‡ë²ˆì§¸", ["ì¡°ê±´", "ìê²©"]),
        ]

    def _build_workflow(self):
        """í‰ê°€ì ì¤‘ì‹¬ ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(RAGState)

        # === ë…¸ë“œ ì •ì˜ ===
        workflow.add_node("analyze_query", self.analyze_query)
        workflow.add_node("rag_search", self.rag_search)  # ëª¨ë“  ì§ˆë¬¸ RAG
        workflow.add_node("decompose_question", self.decompose_question)  # ìƒˆ ë…¸ë“œ
        workflow.add_node("evaluate_coverage", self.evaluate_coverage)  # ìƒˆ ë…¸ë“œ
        workflow.add_node("enhance_partial", self.enhance_partial)  # ìƒˆ ë…¸ë“œ
        workflow.add_node("regenerate_full", self.regenerate_full)  # ìƒˆ ë…¸ë“œ
        workflow.add_node("verify_quality", self.verify_quality)  # ìƒˆ ë…¸ë“œ
        workflow.add_node("format_final", self.format_final)

        # === ì—£ì§€ ì •ì˜ ===

        # ì‹œì‘: ë¶„ì„ â†’ RAG ê²€ìƒ‰
        workflow.set_entry_point("analyze_query")
        workflow.add_edge("analyze_query", "rag_search")

        # RAG ê²€ìƒ‰ â†’ ì§ˆë¬¸ ë¶„í•´
        workflow.add_edge("rag_search", "decompose_question")

        # ì§ˆë¬¸ ë¶„í•´ â†’ í‰ê°€
        workflow.add_edge("decompose_question", "evaluate_coverage")

        # í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸°
        workflow.add_conditional_edges(
            "evaluate_coverage",
            self.route_by_coverage,
            {
                "use_rag": "format_final",  # 80% ì´ìƒ - RAG ê·¸ëŒ€ë¡œ
                "enhance": "enhance_partial",  # 40-79% - ë¶€ë¶„ ë³´ê°•
                "regenerate": "regenerate_full",  # 40% ë¯¸ë§Œ - ì¬ìƒì„±
            },
        )

        # ë³´ê°•/ì¬ìƒì„± â†’ í’ˆì§ˆ ê²€ì¦
        workflow.add_edge("enhance_partial", "verify_quality")
        workflow.add_edge("regenerate_full", "verify_quality")

        # í’ˆì§ˆ ê²€ì¦ í›„ ë¶„ê¸°
        workflow.add_conditional_edges(
            "verify_quality",
            self.route_after_verify,
            {
                "pass": "format_final",
                "retry": "regenerate_full",  # 1íšŒë§Œ ì¬ì‹œë„
                "fallback": "format_final",  # í¬ê¸°
            },
        )

        # ìµœì¢… í¬ë§·íŒ… â†’ ì¢…ë£Œ
        workflow.add_edge("format_final", END)

        return workflow.compile()

    def analyze_query(self, state: RAGState) -> RAGState:
        """ì¿¼ë¦¬ ë¶„ì„ - ì°¨ë‹¨ ë¡œì§ ì œê±°"""
        query = state["query"]
        state["debug_path"] = ["analyze_query"]
        state["iteration"] = 0

        # ì „ì²˜ë¦¬: êµ¬ì–´ì²´ â†’ í‘œì¤€ì–´
        replacements = {
            "ë•Œë ¤ì¹˜": "ìì§„í‡´ì‚¬",
            "ì§¤ë ¸": "í•´ê³  ê¶Œê³ ì‚¬ì§",
            "ë°°ë¯¼": "ë°°ë‹¬ ë¼ì´ë” í”„ë¦¬ëœì„œ",
            "ì¿ íŒ¡": "ì¿ íŒ¡í”Œë ‰ìŠ¤ í”„ë¦¬ëœì„œ",
            "ì–¼ë§ˆ": "ê¸ˆì•¡",
            "ê¹": "ê°ì•¡",
            "ëª»ë°›": "ë¯¸ìˆ˜ê¸‰",  # "ì„ê¸ˆì²´ë¶ˆ"ë¡œ ë³€í™˜ ì•ˆí•¨!
        }

        processed = query.lower()
        for old, new in replacements.items():
            if old in processed:
                processed = processed.replace(old, new)
                logger.info(f"ë³€í™˜: '{old}' â†’ '{new}'")

        # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        query_type = "general"
        for type_name, patterns in self.query_patterns.items():
            if any(p in processed for p in patterns):
                query_type = type_name
                break

        state["processed_query"] = processed
        state["query_type"] = query_type
        state["is_blocked"] = False  # ì°¨ë‹¨ ì•ˆí•¨

        logger.info(f"ë¶„ì„ ì™„ë£Œ: ìœ í˜•={query_type}")
        return state

    def rag_search(self, state: RAGState) -> RAGState:
        """ëª¨ë“  ì§ˆë¬¸ RAG ê²€ìƒ‰"""
        state["debug_path"].append("rag_search")
        query = state["processed_query"]

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        results = self.retriever.retrieve(query, top_k=5)

        state["documents"] = results
        state["relevance_score"] = results[0]["score"] if results else 0.0
        state["context"] = self._build_context(results)

        logger.info(f"RAG ê²€ìƒ‰: {len(results)}ê°œ, ì ìˆ˜={state['relevance_score']:.2f}")
        return state

    def decompose_question(self, state: RAGState) -> RAGState:
        """ì§ˆë¬¸ì„ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ë¶„í•´"""
        state["debug_path"].append("decompose_question")
        query = state["query"]

        requirements = []

        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„í•´
        if "ê°€ëŠ¥" in query or "ë˜ë‚˜ìš”" in query:
            requirements.append("ê°€ëŠ¥_ì—¬ë¶€")

        if "ì–¸ì œ" in query or "ê¸°ê°„" in query:
            requirements.append("ì‹œì _ê¸°ê°„")

        if "ì–¼ë§ˆ" in query or "ê¸ˆì•¡" in query:
            requirements.append("ê¸ˆì•¡_ìˆ˜ì¹˜")

        if "ì–´ë–»ê²Œ" in query or "ë°©ë²•" in query:
            requirements.append("ë°©ë²•_ì ˆì°¨")

        if "ì¡°ê±´" in query or "ìê²©" in query:
            requirements.append("ì¡°ê±´_ìê²©")

        # ë³µì¡í•œ ì§ˆë¬¸ ê°ì§€
        if len(query) > 100 or query.count(",") >= 2:
            requirements.append("ë³µí•©_ìƒí™©")

        # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­
        if not requirements:
            requirements = ["ì¼ë°˜_ì •ë³´"]

        state["requirements"] = requirements
        logger.info(f"ìš”êµ¬ì‚¬í•­ ë¶„í•´: {requirements}")
        return state

    def evaluate_coverage(self, state: RAGState) -> RAGState:
        """RAG ë‹µë³€ì˜ ìš”êµ¬ì‚¬í•­ ì»¤ë²„ë¦¬ì§€ í‰ê°€ - LLM ì‚¬ìš©"""
        state["debug_path"].append("evaluate_coverage")

        documents = state["documents"]
        query = state["query"]

        if not documents:
            state["coverage_score"] = 0.0
            state["needs_enhancement"] = True
            return state

        # ìƒìœ„ 2ê°œ ë¬¸ì„œë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join([doc["text"] for doc in documents[:2]])

        try:
            from openai import OpenAI
            import json
            import config

            client = OpenAI(
                base_url="https://api.together.xyz/v1",
                api_key=config.OPENROUTER_API_KEY,
            )

            # í‰ê°€ í”„ë¡¬í”„íŠ¸
            eval_prompt = f"""ì§ˆë¬¸: {query}
            
    RAG ê²€ìƒ‰ ê²°ê³¼: {context}

    ì´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µí•˜ëŠ”ì§€ 0-1 ì ìˆ˜ë¡œ í‰ê°€í•˜ì„¸ìš”.

    JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”:
    {{"score": 0.85, "sufficient": true}}"""

            completion = client.chat.completions.create(
                model=config.EVAL_MODEL,  # Qwen2.5-7B ì‚¬ìš©
                messages=[{"role": "user", "content": eval_prompt}],
                temperature=0.1,
                max_tokens=100,
            )

            # ì‘ë‹µ íŒŒì‹±
            response = completion.choices[0].message.content
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                state["coverage_score"] = result.get("score", 0.5)
                state["needs_enhancement"] = not result.get("sufficient", False)
            else:
                state["coverage_score"] = 0.5
                state["needs_enhancement"] = True

        except Exception as e:
            logger.error(f"LLM í‰ê°€ ì‹¤íŒ¨: {e}")
            state["coverage_score"] = 0.5
            state["needs_enhancement"] = True

        logger.info(f"LLM í‰ê°€: ì ìˆ˜={state['coverage_score']}")
        return state

    def route_by_coverage(self, state: RAGState) -> str:
        """ì»¤ë²„ë¦¬ì§€ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        score = state["coverage_score"]

        if score >= 0.8:
            return "use_rag"
        elif score >= 0.4:
            return "enhance"
        else:
            return "regenerate"

    def enhance_partial(self, state: RAGState) -> RAGState:
        """ë¶€ë¶„ ë‹µë³€ LLM ë³´ê°•"""
        state["debug_path"].append("enhance_partial")

        try:
            from app import generate_ai_answer

            # ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ ë³´ê°•í•˜ëŠ” í”„ë¡¬í”„íŠ¸
            missing = [req for req, covered in state["coverage"].items() if not covered]

            enhanced_query = f"""
            ì§ˆë¬¸: {state['query']}
            
            í˜„ì¬ ë‹µë³€: {state['documents'][0]['text'] if state['documents'] else ''}
            
            ë‹¤ìŒ ë¶€ë¶„ì„ ë³´ì™„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”: {', '.join(missing)}
            """

            answer = generate_ai_answer(enhanced_query, stream=False)
            state["raw_answer"] = answer
            state["confidence"] = 0.7
            state["answer_method"] = "enhanced"

        except Exception as e:
            logger.error(f"ë³´ê°• ì‹¤íŒ¨: {e}")
            state["raw_answer"] = (
                state["documents"][0]["text"] if state["documents"] else ""
            )
            state["confidence"] = 0.5

        return state

    def regenerate_full(self, state: RAGState) -> RAGState:
        """LLM ì „ì²´ ì¬ìƒì„±"""
        state["debug_path"].append("regenerate_full")
        state["iteration"] += 1

        try:
            from app import generate_ai_answer

            # RAG ê²°ê³¼ í¬í•¨í•´ì„œ ì¬ìƒì„±
            context = state.get("context", "")
            full_query = f"""
            ì§ˆë¬¸: {state['query']}
            
            ì°¸ê³  ì •ë³´:
            {context}
            
            ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
            """

            answer = generate_ai_answer(full_query, stream=False)
            state["raw_answer"] = answer
            state["confidence"] = 0.9
            state["answer_method"] = "regenerated"

        except Exception as e:
            logger.error(f"ì¬ìƒì„± ì‹¤íŒ¨: {e}")
            state["raw_answer"] = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            state["confidence"] = 0.0

        return state

    def verify_quality(self, state: RAGState) -> RAGState:
        """í’ˆì§ˆ ê²€ì¦ (ê°„ë‹¨í•œ ì²´í¬ë§Œ)"""
        state["debug_path"].append("verify_quality")

        answer = state.get("raw_answer", "")

        # ê¸°ë³¸ì ì¸ í’ˆì§ˆ ì²´í¬
        quality_ok = (
            len(answer) > 20 and "ì˜¤ë¥˜" not in answer and state["confidence"] > 0.3
        )

        state["quality_verified"] = quality_ok
        logger.info(f"í’ˆì§ˆ ê²€ì¦: {'í†µê³¼' if quality_ok else 'ì‹¤íŒ¨'}")
        return state

    def route_after_verify(self, state: RAGState) -> str:
        """í’ˆì§ˆ ê²€ì¦ í›„ ë¼ìš°íŒ…"""
        if state.get("quality_verified", False):
            return "pass"
        elif state["iteration"] < 1:  # 1íšŒë§Œ ì¬ì‹œë„
            return "retry"
        else:
            return "fallback"

    def format_final(self, state: RAGState) -> RAGState:
        """ìµœì¢… í¬ë§·íŒ…"""
        state["debug_path"].append("format_final")

        # RAG ë‹µë³€ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ raw_answer
        if not state.get("raw_answer") and state["documents"]:
            state["raw_answer"] = state["documents"][0]["text"]

        answer = state.get("raw_answer", "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        confidence = state.get("confidence", state.get("coverage_score", 0))

        # ì‹ ë¢°ë„ë³„ prefix
        if confidence >= 0.8:
            prefix = "âœ… "
        elif confidence >= 0.5:
            prefix = "ğŸ“Œ "
        else:
            prefix = "â„¹ï¸ "

        state["final_answer"] = prefix + answer

        logger.info(f"ê²½ë¡œ: {' â†’ '.join(state['debug_path'])}")
        logger.info(f"ìµœì¢… ì‹ ë¢°ë„: {confidence:.2f}")

        return state

    def _build_context(self, documents: List[Dict]) -> str:
        """ë¬¸ì„œë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not documents:
            return ""

        context = "ê´€ë ¨ ì •ë³´:\n"
        for i, doc in enumerate(documents[:3], 1):
            context += f"{i}. {doc['text'][:200]}\n"

        return context

    def run(self, query: str) -> Dict:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        initial_state = {
            "query": query,
            "processed_query": "",
            "query_type": "",
            "is_blocked": False,
            "search_strategy": "",
            "documents": [],
            "relevance_score": 0.0,
            "requirements": [],
            "coverage": {},
            "coverage_score": 0.0,
            "needs_enhancement": False,
            "answer_method": "",
            "context": "",
            "raw_answer": "",
            "final_answer": "",
            "confidence": 0.0,
            "iteration": 0,
            "debug_path": [],
        }

        result = self.workflow.invoke(initial_state)

        return {
            "answer": result.get("final_answer", result.get("raw_answer", "")),
            "documents": result.get("documents", []),
            "context": result.get("context", ""),
            "confidence": result.get("confidence", 0.0),
            "debug_path": result.get("debug_path", []),
            "method": result.get("answer_method", "unknown"),
            "coverage_score": result.get("coverage_score", 0.0),
        }


# ê¸°ì¡´ RAGWorkflowë¥¼ ìƒˆ ë²„ì „ìœ¼ë¡œ êµì²´
RAGWorkflow = ImprovedRAGWorkflow
