# backend/rag/pipeline.py (Railwayì—ì„œ ì‹¤í–‰)
import os
import json
import uuid
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import numpy as np
from tqdm import tqdm
from kiwipiepy import Kiwi


class RAGPipeline:
    def __init__(self):
        print("ğŸš€ RAG Pipeline ì´ˆê¸°í™” ì¤‘...")

        # 1. Kiwipiepy ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
        print("ğŸ‡°ğŸ‡· Kiwi í•œêµ­ì–´ ì²˜ë¦¬ê¸° ë¡œë”©...")
        self.kiwi = Kiwi()

        # 2. BGE-M3 ì„ë² ë”© ëª¨ë¸ (embedder.pyì™€ ë™ì¼!)
        print("ğŸ¤– BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        self.embedder = SentenceTransformer("BAAI/bge-m3")

        # 3. Qdrant ì—°ê²° (config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        print("â˜ï¸ Qdrant Cloud ì—°ê²°ì¤‘...")
        try:
            # Railway í™˜ê²½ì—ì„œëŠ” configì—ì„œ
            from config import QDRANT_URL, QDRANT_API_KEY
        except ImportError:
            # ë¡œì»¬ í…ŒìŠ¤íŠ¸ì‹œ í™˜ê²½ë³€ìˆ˜
            QDRANT_URL = os.getenv("QDRANT_URL")
            QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

        self.qdrant = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

        print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì¤€ë¹„ ì™„ë£Œ!\n")

    def parse_documents(self, file_paths: List[str]) -> List[Dict]:
        """ë¬¸ì„œ íŒŒì‹± - ì‹¬í”Œ ë²„ì „"""
        all_texts = []

        for file_path in file_paths:
            print(f"ğŸ“– íŒŒì‹± ì¤‘: {file_path}")

            if file_path.endswith(".json"):
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for item in data.get("faqs", []):
                        text = f"ì§ˆë¬¸: {item['q']}\në‹µë³€: {item['a']}"

                        all_texts.append(
                            {
                                "text": text,
                                "metadata": {
                                    "source": "knowledge.json",
                                    "category": item.get("category", ""),
                                    "keywords": item.get("keywords", []),
                                },
                            }
                        )

        print(f"âœ… {len(all_texts)}ê°œ ë¬¸ì„œ íŒŒì‹± ì™„ë£Œ")
        return all_texts

    def chunk_texts(self, documents: List[Dict], max_size: int = 500) -> List[Dict]:
        """í•œêµ­ì–´ ê¸°ë°˜ ì²­í‚¹"""
        chunks = []

        print("âœ‚ï¸ í•œêµ­ì–´ ì²­í‚¹ ì‹œì‘...")
        for doc in tqdm(documents, desc="ì²­í‚¹"):
            text = doc["text"]

            # FAQëŠ” ì´ë¯¸ êµ¬ì¡°í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
            if "ì§ˆë¬¸:" in text and "ë‹µë³€:" in text:
                chunks.append(doc)
                continue

            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            if len(text) > max_size:
                sentences = self.kiwi.split_into_sents(text)
                current_chunk = ""
                current_size = 0

                for sent in sentences:
                    sent_text = sent.text
                    if current_size + len(sent_text) > max_size:
                        if current_chunk:
                            chunks.append(
                                {"text": current_chunk, "metadata": doc["metadata"]}
                            )
                        current_chunk = sent_text
                        current_size = len(sent_text)
                    else:
                        current_chunk += " " + sent_text
                        current_size += len(sent_text)

                if current_chunk:
                    chunks.append({"text": current_chunk, "metadata": doc["metadata"]})
            else:
                chunks.append(doc)

        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunks

    def embed_chunks(self, chunks: List[Dict]) -> np.ndarray:
        """ì„ë² ë”© ìƒì„±"""
        print("ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        texts = [chunk["text"] for chunk in chunks]

        # ë°°ì¹˜ ì²˜ë¦¬
        embeddings = self.embedder.encode(
            texts, normalize_embeddings=True, show_progress_bar=True, batch_size=32
        )

        print(f"âœ… {embeddings.shape} ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        return embeddings

    def upload_to_qdrant(self, chunks: List[Dict], embeddings: np.ndarray):
        """Qdrant Cloud ì—…ë¡œë“œ"""
        print("â˜ï¸ Qdrant Cloud ì—…ë¡œë“œ ì¤‘...")

        collection_name = "unemployment_rag"

        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        try:
            collections = self.qdrant.get_collections()
            exists = any(c.name == collection_name for c in collections.collections)

            if exists:
                print("âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ë°œê²¬")
                # ê¸°ì¡´ ë°ì´í„° í™•ì¸
                count = self.qdrant.count(collection_name=collection_name)
                print(f"  í˜„ì¬ {count.count}ê°œ ë²¡í„° ì¡´ì¬")

                # ì‚­ì œ ì—¬ë¶€ í™•ì¸
                response = input("ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“¤ê¹Œìš”? (y/n): ")
                if response.lower() == "y":
                    self.qdrant.delete_collection(collection_name)
                    print("ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ")
                else:
                    print("ê¸°ì¡´ ì»¬ë ‰ì…˜ ìœ ì§€, ì—…ë¡œë“œ ì¤‘ë‹¨")
                    return
        except:
            pass

        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (1024ì°¨ì›)
        self.qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
        )
        print("ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (1024ì°¨ì›)")

        # í¬ì¸íŠ¸ ìƒì„±
        points = []
        for i, chunk in enumerate(chunks):
            points.append(
                PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embeddings[i].tolist(),
                    payload={"text": chunk["text"], **chunk["metadata"]},
                )
            )

        # ì—…ë¡œë“œ
        self.qdrant.upsert(collection_name=collection_name, points=points)

        print(f"âœ… {len(points)}ê°œ ë²¡í„° ì—…ë¡œë“œ ì™„ë£Œ!")

    def run(self, file_paths: List[str] = None):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 50)
        print("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 50)

        # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ
        if file_paths is None:
            file_paths = ["backend/data/knowledge.json"]

        # 1. íŒŒì‹±
        documents = self.parse_documents(file_paths)

        # 2. ì²­í‚¹
        chunks = self.chunk_texts(documents)

        # 3. ì„ë² ë”©
        embeddings = self.embed_chunks(chunks)

        # 4. ì—…ë¡œë“œ
        self.upload_to_qdrant(chunks, embeddings)

        print("\n" + "=" * 50)
        print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… í†µê³„:")
        print(f"  - ë¬¸ì„œ: {len(documents)}ê°œ")
        print(f"  - ì²­í¬: {len(chunks)}ê°œ")
        print(f"  - ë²¡í„°: {embeddings.shape}")
        print("=" * 50)


# Railwayì—ì„œ ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥
if __name__ == "__main__":
    import sys

    # ê²½ë¡œ ì„¤ì •
    if "backend" not in sys.path:
        sys.path.insert(0, "backend")

    pipeline = RAGPipeline()

    # Railway í™˜ê²½ì—ì„œ knowledge.json ê²½ë¡œ
    files = ["backend/data/knowledge.json"]

    pipeline.run(files)
