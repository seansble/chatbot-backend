# backend/rag/vectorstore.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import uuid
from typing import List, Dict
import numpy as np
import logging
from rank_bm25 import BM25Okapi
import os
import json
import hashlib
from pathlib import Path
from .tokenizer import KiwiTokenizer

logger = logging.getLogger(__name__)


class QdrantVectorStore:
    def __init__(self, collection_name: str = "unemployment_rag"):
        """Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # í™˜ê²½ë³€ìˆ˜ ì²´í¬
        qdrant_url = os.getenv("QDRANT_URL")
        qdrant_api_key = os.getenv("QDRANT_API_KEY")

        logger.info(
            f"Environment check - URL: {qdrant_url[:30] if qdrant_url else 'None'}"
        )
        logger.info(f"Environment check - Key: {'Set' if qdrant_api_key else 'None'}")

        if qdrant_url and qdrant_api_key:
            # Railway/Production - Qdrant Cloud
            try:
                self.client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
                logger.info("âœ… Qdrant Cloud ëª¨ë“œ ì—°ê²° ì‹œë„")
            except Exception as e:
                logger.error(f"Qdrant Cloud ì—°ê²° ì‹¤íŒ¨: {e}")
                # í´ë°±ìœ¼ë¡œ ë¡œì»¬ ì‹œë„
                self.client = QdrantClient(host="localhost", port=6333)
        else:
            # ë¡œì»¬ ê°œë°œ - Docker Qdrant
            self.client = QdrantClient(host="localhost", port=6333)
            logger.info("âœ… Qdrant Docker ëª¨ë“œ ì—°ê²° ì„±ê³µ")

        self.collection_name = collection_name

        # Kiwi í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = KiwiTokenizer()

        # BM25ë¥¼ ìœ„í•œ ë¬¸ì„œ ì €ì¥
        self.documents = []
        self.tokenized_docs = []
        self.doc_keys = []
        self.bm25 = None

        # BM25 ì´ˆê¸°í™” ì‹œë„
        try:
            # self._load_all_documents()  # ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œ ë¹„í™œì„±í™”
            self.documents = []
            self.tokenized_docs = []
            self.doc_keys = []
            self.bm25 = None
            logger.info("ì´ˆê¸° ë¬¸ì„œ ë¡œë“œ ê±´ë„ˆëœ€ - ê²€ìƒ‰ ì‹œ ë¡œë“œë¨")
        except Exception as e:
            logger.warning(f"BM25 ì´ˆê¸°í™” ì‹¤íŒ¨ (ì •ìƒ): {e}")
            self.documents = []
            self.tokenized_docs = []
            self.doc_keys = []
            self.bm25 = None

    def _load_all_documents(self):
        """ëª¨ë“  ë¬¸ì„œ ë¡œë“œ ë° BM25 ì´ˆê¸°í™”"""
        try:
            # ë¨¼ì € knowledge_kiwi.jsonì´ ìˆëŠ”ì§€ í™•ì¸
            knowledge_path = (
                Path(__file__).parent.parent / "data" / "knowledge_kiwi.json"
            )

            if knowledge_path.exists():
                # knowledge_kiwi.jsonì—ì„œ ë¡œë“œ
                logger.info(f"ğŸ“‚ Loading from {knowledge_path}")
                with open(knowledge_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                self.documents = []
                self.tokenized_docs = []
                self.doc_keys = []

                # knowledge_kiwi.json êµ¬ì¡°: {"knowledge": [...], "metadata": {...}}
                if isinstance(data, dict) and "knowledge" in data:
                    knowledge_items = data["knowledge"]
                else:
                    knowledge_items = data if isinstance(data, list) else []

                for item in knowledge_items:
                    # ê° itemì´ dictì¸ì§€ í™•ì¸
                    if not isinstance(item, dict):
                        logger.warning(f"Skipping non-dict item: {type(item)}")
                        continue

                    doc_key = item.get("doc_key")
                    if not doc_key:
                        doc_key = hashlib.sha1(item["text"].encode("utf-8")).hexdigest()

                    doc = {
                        "id": item.get("id", str(uuid.uuid4())),
                        "doc_key": doc_key,
                        "text": item.get("text", ""),
                        "metadata": {
                            "category": item.get("category", ""),
                            "keywords": item.get("keywords", []),
                            "priority": item.get("priority", 5),
                            "question": item.get("question", ""),
                            "answer": item.get("answer", ""),
                        },
                        "source": item.get("source", "knowledge_faq"),
                    }
                    self.documents.append(doc)
                    self.doc_keys.append(doc_key)

                    # ì´ë¯¸ í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©
                    if "tokens" in item and isinstance(item["tokens"], list):
                        tokens = item["tokens"]
                    else:
                        # ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ í† í°í™”
                        tokens = self.tokenizer.tokenize(doc["text"])

                    # ë¹ˆ í† í° ì²˜ë¦¬
                    if not tokens:
                        tokens = ["[EMPTY]"]

                    self.tokenized_docs.append(tokens)

                logger.info(
                    f"âœ… knowledge_kiwi.jsonì—ì„œ {len(self.documents)}ê°œ ë¬¸ì„œ ë¡œë“œ"
                )

            else:
                # Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë°©ì‹)
                logger.info("knowledge_kiwi.json ì—†ìŒ - Qdrantì—ì„œ ë¡œë“œ")
                result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=1000,
                    with_payload=True,
                    with_vectors=False,
                )

                self.documents = []
                self.tokenized_docs = []
                self.doc_keys = []

                for point in result[0]:
                    doc_key = point.payload.get("doc_key")
                    if not doc_key:
                        doc_key = hashlib.sha1(
                            point.payload.get("text", "").encode("utf-8")
                        ).hexdigest()

                    doc = {
                        "id": point.id,
                        "doc_key": doc_key,
                        "text": point.payload.get("text", ""),
                        "metadata": point.payload.get("metadata", {}),
                        "source": point.payload.get("source", ""),
                    }
                    self.documents.append(doc)
                    self.doc_keys.append(doc_key)

                    # Kiwië¡œ í† í°í™”
                    tokens = self.tokenizer.tokenize(doc["text"])

                    if not isinstance(tokens, list):
                        tokens = []
                    if not tokens:
                        tokens = ["[EMPTY]"]

                    self.tokenized_docs.append(tokens)

            # BM25 ì´ˆê¸°í™”
            if self.tokenized_docs and all(
                isinstance(doc, list) for doc in self.tokenized_docs
            ):
                self.bm25 = BM25Okapi(self.tokenized_docs)
                logger.info(
                    f"âœ… BM25 Kiwi í† í¬ë‚˜ì´ì €ë¡œ ì´ˆê¸°í™”: {len(self.documents)}ê°œ ë¬¸ì„œ"
                )

                # ì²« 3ê°œ ë¬¸ì„œì˜ í† í° ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                for i in range(min(3, len(self.tokenized_docs))):
                    logger.debug(
                        f"ë¬¸ì„œ {i+1} í† í° ìƒ˜í”Œ: {self.tokenized_docs[i][:10]}..."
                    )

            else:
                logger.error("BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: í† í° í˜•ì‹ ì˜¤ë¥˜")
                self.bm25 = None

        except Exception as e:
            logger.error(f"BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback

            logger.error(traceback.format_exc())
            self.documents = []
            self.tokenized_docs = []
            self.doc_keys = []
            self.bm25 = None

    def create_collection(self, dimension: int):
        """ì»¬ë ‰ì…˜ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
            collections = self.client.get_collections().collections
            if any(c.name == self.collection_name for c in collections):
                logger.info(f"ì»¬ë ‰ì…˜ '{self.collection_name}' ì´ë¯¸ ì¡´ì¬")
                return

            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=dimension, distance=Distance.COSINE),
            )
            logger.info(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")

    def add_documents(self, documents: List[Dict], embeddings: np.ndarray):
        """ë¬¸ì„œ ì¶”ê°€"""
        points = []

        for i, doc in enumerate(documents):
            point_id = str(uuid.uuid4())

            # doc_key ì¶”ê°€
            doc_key = doc.get("doc_key")
            if not doc_key:
                doc_key = hashlib.sha1(doc["text"].encode("utf-8")).hexdigest()

            point = PointStruct(
                id=point_id,
                vector=embeddings[i].tolist(),
                payload={
                    "text": doc["text"],
                    "doc_key": doc_key,
                    "metadata": doc.get("metadata", {}),
                    "source": doc.get("source", "unknown"),
                },
            )
            points.append(point)

        self.client.upsert(collection_name=self.collection_name, points=points)
        logger.info(f"âœ… {len(points)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")

        # BM25 ì—…ë°ì´íŠ¸
        self._load_all_documents()

    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰"""
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding.tolist(),
            limit=top_k,
        )

        return [
            {
                "text": hit.payload["text"],
                "doc_key": hit.payload.get("doc_key"),
                "metadata": hit.payload.get("metadata", {}),
                "score": hit.score,
            }
            for hit in results
        ]

    def bm25_search(self, query: str, top_k: int = 10) -> List[Dict]:
        """BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ - Kiwi í† í¬ë‚˜ì´ì € ì‚¬ìš©"""
        if not self.bm25 or not self.documents:
            logger.warning("BM25 ì‚¬ìš© ë¶ˆê°€ - ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return []

        try:
            # 1. ê¸°ë³¸ í† í°í™”
            query_tokens = self.tokenizer.tokenize(query)

            # 2. ì¿¼ë¦¬ë§Œ ë™ì˜ì–´ í™•ì¥
            expanded_query = self.tokenizer.expand_query(query_tokens)

            # íƒ€ì… ì²´í¬ - ë°˜ë“œì‹œ list[str]
            if not isinstance(expanded_query, list):
                logger.error(f"BM25 ì¿¼ë¦¬ íƒ€ì… ì˜¤ë¥˜: {type(expanded_query)}")
                return []

            # ë¹ˆ ì¿¼ë¦¬ ì²˜ë¦¬
            if not expanded_query:
                expanded_query = ["[QUERY]"]

            # ë¬¸ìì—´ë§Œ í—ˆìš©
            expanded_query = [
                str(token) for token in expanded_query if isinstance(token, str)
            ]

            logger.info(f"BM25 ì¿¼ë¦¬ í† í°: {expanded_query[:10]}")

            # 3. BM25 ê²€ìƒ‰
            scores = self.bm25.get_scores(expanded_query)

            # ì ìˆ˜ì™€ ë¬¸ì„œ ì¸ë±ìŠ¤ ìŒ
            doc_scores = [(score, idx) for idx, score in enumerate(scores)]
            doc_scores.sort(reverse=True, key=lambda x: x[0])

            results = []
            for score, idx in doc_scores[:top_k]:
                if score > 0:
                    doc = self.documents[idx].copy()
                    doc["bm25_score"] = float(score)
                    doc["score"] = float(score)
                    results.append(doc)

            logger.info(f"BM25 ê²€ìƒ‰: {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬")

            # ìƒìœ„ 3ê°œ ê²°ê³¼ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            for i, result in enumerate(results[:3]):
                doc_key = result.get("doc_key")
                doc_key_display = doc_key[:8] + "..." if doc_key else "N/A"

                logger.debug(
                    f"BM25 ê²°ê³¼ {i+1} - doc_key: {doc_key_display}, "
                    f"score: {result.get('score', 0):.3f}"
                )

            return results

        except Exception as e:
            logger.error(f"BM25 ê²€ìƒ‰ ì¤‘ ì—ëŸ¬: {e}")
            return []

    def rebuild_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ìˆ˜ë™ í˜¸ì¶œìš©)"""
        logger.info("ğŸ”„ BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘...")
        self._load_all_documents()
        if self.bm25:
            logger.info("âœ… BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")
        else:
            logger.error("âŒ BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨")

    def get_query_tags(self, query: str) -> set:
        """ì§ˆì˜ì—ì„œ ê´€ë ¨ íƒœê·¸ ì¶”ì¶œ"""
        TAG_MAP = {
            "ìœ¡ì•„íœ´ì§": {"ìœ¡ì•„íœ´ì§", "ë³µì§", "ê²½ë ¥ë‹¨ì ˆ", "ì¶œì‚°"},
            "ì„ê¸ˆì²´ë¶ˆ": {"ì„ê¸ˆì²´ë¶ˆ", "ì²´ë¶ˆí™•ì¸ì„œ", "ë…¸ë™ë¶€"},
            "ê¶Œê³ ì‚¬ì§": {"ê¶Œê³ ì‚¬ì§", "êµ¬ì¡°ì¡°ì •", "í•´ê³ "},
            "ì‹¤ì—…ê¸‰ì—¬": {"ì‹¤ì—…ê¸‰ì—¬", "ìˆ˜ê¸‰ìê²©", "ê³ ìš©ë³´í—˜"},
            "ê³„ì•½ë§Œë£Œ": {"ê³„ì•½ë§Œë£Œ", "ê³„ì•½ì§", "ë¹„ìë°œì "},
            "ìì§„í‡´ì‚¬": {"ìì§„í‡´ì‚¬", "ìë°œì ", "ì •ë‹¹í•œì‚¬ìœ "},
            "êµ¬ì§í™œë™": {"êµ¬ì§í™œë™", "ì‹¤ì—…ì¸ì •", "ì›Œí¬ë„·"},
        }

        tags = set()
        for keyword, related_tags in TAG_MAP.items():
            if keyword in query:
                tags |= related_tags
        return tags

    def hybrid_search(
        self, query: str, query_embedding: np.ndarray, top_k: int = 10
    ) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - RRF ê¸°ë°˜ ê²°í•© (doc_key ì‚¬ìš©)"""

        # BM25 ì²« ê²€ìƒ‰ ì‹œ ìë™ ë¡œë“œ
        if self.bm25 is None or not self.documents:
            logger.info("BM25 ë¯¸ì´ˆê¸°í™” - ì²« ê²€ìƒ‰ ì‹œ ë¡œë“œ ì¤‘...")
            try:
                self._load_all_documents()
                if self.bm25:
                    logger.info(f"âœ… BM25 ë¡œë“œ ì„±ê³µ: {len(self.documents)}ê°œ ë¬¸ì„œ")
                else:
                    logger.warning("BM25 ë¡œë“œ ì‹¤íŒ¨ - ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©")
            except Exception as e:
                logger.error(f"BM25 ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")

        # 1. ê°ê° ê²€ìƒ‰ (ë” ë§ì´ ìˆ˜ì§‘)
        bm25_results = self.bm25_search(query, top_k=50) if self.bm25 else []
        vector_results = self.search(query_embedding, top_k=100)

        # 2. doc_key ê¸°ë°˜ RRF ìŠ¤ì½”ì–´ ê³„ì‚°
        k = 10

        # doc_key -> (rank, doc) ë§¤í•‘
        bm25_hits = {}
        for i, doc in enumerate(bm25_results):
            doc_key = doc.get("doc_key")
            if not doc_key:
                doc_key = hashlib.sha1(doc["text"].encode()).hexdigest()
            bm25_hits[doc_key] = (i + 1, doc)

        vector_hits = {}
        for i, doc in enumerate(vector_results):
            doc_key = doc.get("doc_key")
            if not doc_key:
                doc_key = hashlib.sha1(doc["text"].encode()).hexdigest()
            vector_hits[doc_key] = (i + 1, doc)

        # RRF ìŠ¤ì½”ì–´ ê³„ì‚°
        def rrf(rank, k=10):
            return 1 / (k + rank)

        bm25_rrf = {key: rrf(rank) for key, (rank, _) in bm25_hits.items()}
        vector_rrf = {key: rrf(rank) for key, (rank, _) in vector_hits.items()}

        # 3. ì¿¼ë¦¬ íƒ€ì… íŒë‹¨
        query_tokens = self.tokenizer.tokenize(query)
        token_count = len(query_tokens)

        has_numbers = any(
            token.replace("ë…„", "").replace("ì›”", "").replace("ì¼", "").isdigit()
            for token in query_tokens
        )

        if token_count <= 4 and has_numbers:
            alpha = 0.5
        elif token_count > 10:
            alpha = 0.7
        else:
            alpha = 0.6

        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ - í† í°ìˆ˜: {token_count}, alpha: {alpha}")

        # 4. doc_key ê¸°ë°˜ ìŠ¤ì½”ì–´ ê²°í•©
        all_keys = set(bm25_rrf.keys()) | set(vector_rrf.keys())
        combined = {}
        all_docs = {}

        for doc_key in all_keys:
            # ìŠ¤ì½”ì–´ ê²°í•©
            bm25_score = bm25_rrf.get(doc_key, 0)
            vector_score = vector_rrf.get(doc_key, 0)

            score = alpha * vector_score + (1 - alpha) * bm25_score

            # ì–‘ì¸¡ êµì§‘í•© ë³´ë„ˆìŠ¤ (ë¨¼ì € ì ìš©)
            if doc_key in bm25_rrf and doc_key in vector_rrf:
                score += 0.02

            combined[doc_key] = score

            # ë¬¸ì„œ ì •ë³´ ì €ì¥ (í•œìª½ì—ì„œë§Œ ê°€ì ¸ì˜¤ê¸°)
            if doc_key in bm25_hits:
                all_docs[doc_key] = bm25_hits[doc_key][1]
            elif doc_key in vector_hits:
                all_docs[doc_key] = vector_hits[doc_key][1]

        # 5. ì§ˆì˜ ì í•©ë„ ê¸°ë°˜ ë³´ë„ˆìŠ¤ ê³„ì‚°
        query_tags = self.get_query_tags(query)
        important_phrases = {
            "ìœ¡ì•„íœ´ì§",
            "ì„ê¸ˆì²´ë¶ˆ",
            "ê¶Œê³ ì‚¬ì§",
            "ê³„ì•½ë§Œë£Œ",
            "ì‹¤ì—…ê¸‰ì—¬",
            "ìì§„í‡´ì‚¬",
            "êµ¬ì§í™œë™",
        }
        query_phrases = {p for p in important_phrases if p in query}

        for doc_key in combined.keys():
            doc = all_docs[doc_key]
            metadata = doc.get("metadata", {})
            doc_keywords = set(metadata.get("keywords", []))

            bonus = 0.0

            # 1) íƒœê·¸ êµì§‘í•© ë³´ë„ˆìŠ¤ (ë©”ì¸)
            overlap = len(query_tags & doc_keywords)
            if overlap >= 2:
                bonus += 0.06
                logger.debug(f"íƒœê·¸ êµì§‘í•© {overlap}ê°œ: {query_tags & doc_keywords}")
            elif overlap == 1:
                bonus += 0.04

            # 2) í”„ë ˆì´ì¦ˆ ì •í™• ë§¤ì¹­
            if any(phrase in doc["text"] for phrase in query_phrases):
                bonus += 0.03
                logger.debug(f"í”„ë ˆì´ì¦ˆ ë§¤ì¹­: {query_phrases}")

            # 3) ì „ì—­ priority (ìµœì†Œí™”)
            priority = metadata.get("priority", 5)
            if priority >= 9:
                bonus += 0.02
            elif priority >= 7:
                bonus += 0.01

            # ì´í•© ì œí•œ
            bonus = min(bonus, 0.09)
            combined[doc_key] += bonus

            if bonus > 0:
                logger.debug(
                    f"ë³´ë„ˆìŠ¤ ì ìš© - ë¬¸ì„œ: {doc['text'][:30]}, "
                    f"íƒœê·¸êµì§‘í•©: {overlap}, í”„ë ˆì´ì¦ˆ: {bool(query_phrases & {p for p in query_phrases if p in doc['text']})}, "
                    f"priority: {priority}, ì´ë³´ë„ˆìŠ¤: {bonus:.3f}"
                )

        # 6. ì •ë ¬
        sorted_results = sorted(combined.items(), key=lambda x: x[1], reverse=True)

        # 7. ê²°ê³¼ ìƒì„±
        results = []
        seen_keys = set()

        # ìƒìœ„ ê²°ê³¼ ì¶”ê°€
        for doc_key, score in sorted_results[:top_k]:
            doc = all_docs[doc_key].copy()
            doc["hybrid_score"] = score
            doc["bm25_rrf"] = bm25_rrf.get(doc_key, 0)
            doc["vector_rrf"] = vector_rrf.get(doc_key, 0)
            results.append(doc)
            seen_keys.add(doc_key)

        # 8. êµì°¨ ë³´ì¥ - BM25 1ìœ„ì™€ Vector 1ìœ„ ê°•ì œ í¬í•¨
        if bm25_results and len(results) >= 5:
            bm25_top_key = list(bm25_hits.keys())[0] if bm25_hits else None
            if bm25_top_key and bm25_top_key not in seen_keys:
                doc = bm25_hits[bm25_top_key][1].copy()
                doc["hybrid_score"] = combined.get(bm25_top_key, 0)
                doc["bm25_rrf"] = bm25_rrf.get(bm25_top_key, 0)
                doc["vector_rrf"] = 0
                results.insert(4, doc)

        if vector_results and len(results) >= 6:
            vector_top_key = list(vector_hits.keys())[0] if vector_hits else None
            if vector_top_key and vector_top_key not in seen_keys:
                doc = vector_hits[vector_top_key][1].copy()
                doc["hybrid_score"] = combined.get(vector_top_key, 0)
                doc["bm25_rrf"] = 0
                doc["vector_rrf"] = vector_rrf.get(vector_top_key, 0)
                results.insert(5, doc)

        # ìµœì¢… top_k ìœ ì§€
        results = results[:top_k]

        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")

        # ë””ë²„ê·¸ ë¡œê¹…
        for i, result in enumerate(results[:3]):
            doc_key = result.get("doc_key")
            doc_key_display = doc_key[:8] + "..." if doc_key else "N/A"

            logger.debug(
                f"í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ {i+1} - doc_key: {doc_key_display}, "
                f"hybrid: {result.get('hybrid_score', 0):.3f}, "
                f"bm25_rrf: {result.get('bm25_rrf', 0):.3f}, "
                f"vector_rrf: {result.get('vector_rrf', 0):.3f}"
            )

        return results
